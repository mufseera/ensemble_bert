{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacc581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = [[\"What is the capital of Russia?\", \"The capital of Russia is Moscow.\", 1],\n",
    "           [\"What is the capital of India?\", \"The capital of Russia is Delhi.\", 1],\n",
    "           [\"What is the capital of United States?\", \"The capital of Russia is Washington.\", 1], \n",
    "           [\"What is the capital of Germany?\", \"The capital of Russia is Berlin.\", 1],\n",
    "           [\"What is the capital of France?\", \"The capital of Russia is Paris.\", 1],\n",
    "           [\"What is the capital of Russia?\", \"Goku loves chi chi.\", 0],\n",
    "           [\"What is the capital of India?\", \"Gohan is better than Goku for sure.\", 0],\n",
    "           [\"What is the capital of United States?\", \"Freeza has to freeze.\", 0], \n",
    "           [\"What is the capital of Germany?\", \"Einstien should have nuked Hitler.\", 0],\n",
    "           [\"What is the capital of France?\", \"Newton lost it when the apple fell on his head.\", 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbabca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertPreTrainedModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "601b1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_Bert(BertPreTrainedModel):\n",
    "    def __init__(self,config,*args,**kwargs):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        #first model\n",
    "        self.bert_model_1 = BertModel(config)\n",
    "        #second model\n",
    "        self.bert_model_2 = BertModel(config)\n",
    "        \n",
    "        self.cls = nn.Linear(2*self.config.hidden_size,2)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self,\n",
    "               input_ids=None,\n",
    "               attention_mask=None,\n",
    "               token_type_ids=None,\n",
    "               next_sentence_label=None):\n",
    "        \n",
    "        outputs = []\n",
    "        input_ids_1 = input_ids[0]\n",
    "        attention_mask_1 = attention_mask[0]\n",
    "        outputs.append(self.bert_model_1(input_ids_1,\n",
    "                                         attention_mask=attention_mask_1))\n",
    "        input_ids_2 = input_ids[1]\n",
    "        attention_mask_2 = attention_mask[0]\n",
    "        outputs.append(self.bert_model_2(input_ids_2,\n",
    "                                         attention_mask=attention_mask_2))\n",
    "        \n",
    "        last_hidden_state =torch.cat([output[1] for output in outputs],dim=1)\n",
    "        logits = self.cls(last_hidden_state)\n",
    "        \n",
    "        if next_sentence_label is not None:\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            l = loss(logits.view(-1,2),next_sentence_label.view(-1))\n",
    "            return l,logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bf3e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mufseeramusthafa/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig()\n",
    "model = Ensemble_Bert(config)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "lr = 1e-5\n",
    "no_decay =[\"bias\",\"LayerNorm.weight\"]\n",
    "param = [p for n,p in model.named_parameters()if not  any(nd in n for nd in no_decay)]\n",
    "optimizer = AdamW(param,lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e26b7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_dataset(dataset,qa=True):\n",
    "    input_ids,attention_mask = [],[]\n",
    "    labels = []\n",
    "    for point in dataset:\n",
    "        if qa is True:\n",
    "            q,a,_ = point\n",
    "        else:\n",
    "            a,q,_ = point\n",
    "        inputs = tokenizer.encode_plus(q,a,\n",
    "                             return_tensors=\"pt\",\n",
    "                             max_length=128,\n",
    "                             truncation=True,\n",
    "                             padding=\"max_length\")\n",
    "        input_ids.append(inputs[\"input_ids\"])\n",
    "        attention_mask.append(inputs[\"attention_mask\"])\n",
    "        labels.append(point[-1])\n",
    "    input_ids = torch.cat(input_ids,dim=0)\n",
    "    attention_mask = torch.cat(attention_mask,dim=0)\n",
    "    return input_ids,attention_mask,labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b612241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pytorch(Dataset):\n",
    "    def __init__(self,input_ids,attention_mask,labels=None):\n",
    "        self.input_ids = np.array(input_ids)\n",
    "        self.attention_mask = np.array(attention_mask)\n",
    "        self.labels = torch.tensor(labels,dtype=torch.long)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx],self.attention_mask[idx],self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32c1c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_qa,attention_mask_qa,label_qa = qa_dataset(datapoints)\n",
    "data_set_qa = dataset_pytorch(input_ids_qa,attention_mask_qa,label_qa)\n",
    "dataloader_qa = DataLoader(dataset=data_set_qa,\n",
    "                           batch_size=5,sampler=SequentialSampler(data_set_qa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d927bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_aq,attention_mask_aq,labels_aq = qa_dataset(datapoints,qa=False)\n",
    "dataset_aq = dataset_pytorch(input_ids_aq,attention_mask_aq,labels_aq)\n",
    "dataloader_aq = DataLoader(dataset=dataset_aq,\n",
    "                           batch_size=5,\n",
    "                          sampler=SequentialSampler(dataset_aq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6ea6f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 :loss1.0444568395614624\n",
      "epoch0 :loss0.3622767925262451\n",
      "\n",
      "\n",
      "epoch1 :loss0.8111569285392761\n",
      "epoch1 :loss0.55194491147995\n",
      "\n",
      "\n",
      "epoch2 :loss0.43230146169662476\n",
      "epoch2 :loss0.8224819898605347\n",
      "\n",
      "\n",
      "epoch3 :loss0.34562668204307556\n",
      "epoch3 :loss0.8164757490158081\n",
      "\n",
      "\n",
      "epoch4 :loss0.38298362493515015\n",
      "epoch4 :loss0.7245930433273315\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "    for step, combined_data in enumerate(zip(dataloader_qa,dataloader_aq)):\n",
    "        batch_1,batch_2 = combined_data\n",
    "        model.train()\n",
    "        inputs = {\n",
    "            \"input_ids\" : [batch_1[0],batch_2[0]],\n",
    "            \"attention_mask\" : [batch_1[1],batch_1[1]],\n",
    "            \"next_sentence_label\" : batch_2[2]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        print(f\"epoch{i} :loss{loss}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7e942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81332d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84b439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
